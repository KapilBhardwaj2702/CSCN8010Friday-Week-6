{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf4c46a",
   "metadata": {},
   "source": [
    "# Statistical Classification Log-Loss (Cross-Entropy)\n",
    "### Use Case: Hours Studied vs. Pass/Fail Prediction\n",
    "This notebook demonstrates how to calculate statistical classification Log-Loss (Cross-Entropy Loss) in a binary classification scenario using a logistic regression example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb34df",
   "metadata": {},
   "source": [
    "## ðŸ“˜ What is Log-Loss (Cross-Entropy)?\n",
    "Log-Loss measures the performance of a classification model where the prediction is a probability value between 0 and 1. A lower Log-Loss indicates a better performing model.\n",
    "\n",
    "**Binary Cross-Entropy Formula:**\n",
    "\\[\\text{LogLoss} = -[y \\log(p) + (1 - y) \\log(1 - p)]\\]\n",
    "- `y` is the actual class label (0 or 1)\n",
    "- `p` is the predicted probability of class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f238ea",
   "metadata": {},
   "source": [
    "## ðŸ§ª Generate Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac119e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create synthetic data\n",
    "np.random.seed(42)\n",
    "hours_studied = np.random.uniform(0, 10, 100)\n",
    "actual_pass = (hours_studied + np.random.normal(0, 2, 100)) > 5\n",
    "actual_pass = actual_pass.astype(int)\n",
    "\n",
    "df = pd.DataFrame({'Hours_Studied': hours_studied, 'Pass': actual_pass})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df['Hours_Studied'], df['Pass'], alpha=0.7, c=df['Pass'], cmap='bwr')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Pass (1) / Fail (0)')\n",
    "plt.title('Hours Studied vs. Pass/Fail')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb16ed",
   "metadata": {},
   "source": [
    "## ðŸ”„ Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d540c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Example usage:\n",
    "z = np.linspace(-10, 10, 100)\n",
    "plt.plot(z, sigmoid(z))\n",
    "plt.title('Sigmoid Function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deee31a",
   "metadata": {},
   "source": [
    "## ðŸ§  Predicting Probabilities with Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple logistic model using one feature (Hours Studied)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "X = df[['Hours_Studied']]\n",
    "y = df['Pass']\n",
    "model.fit(X, y)\n",
    "\n",
    "pred_probs = model.predict_proba(X)[:, 1]  # Probability of class 1 (pass)\n",
    "df['Predicted_Prob'] = pred_probs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d21c2b",
   "metadata": {},
   "source": [
    "## ðŸ“‰ Implementing Log-Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9945ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15  # Avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Calculate log-loss on our predictions\n",
    "log_loss_value = binary_cross_entropy(df['Pass'], df['Predicted_Prob'])\n",
    "print(f'Log-Loss: {log_loss_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71ad42",
   "metadata": {},
   "source": [
    "## ðŸŽ¤ Key Talking Points\n",
    "1. **Why Log-Loss Matters**: Log-Loss penalizes confident but wrong predictions more than mild wrong guesses. This encourages better probability calibration in models.\n",
    "2. **Hours Studied â€“ A Realistic Feature**: Education is a relatable domain. Logistic regression maps 'hours studied' to the probability of passing, simulating a real-world decision model.\n",
    "3. **Sigmoid + Cross Entropy**: These are the core components of binary classification. The sigmoid maps inputs to probabilities, and cross-entropy quantifies the 'cost' of wrong predictions."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
